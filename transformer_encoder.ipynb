{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsKX8yM5aDpc",
        "outputId": "ca5491a9-6188-40cc-ea08-118334ad5ea3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install OpenNMT-py\n",
        "#!pip install -r requirements.opt.txt"
      ],
      "metadata": {
        "id": "Hzc4rtfvdBgB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "C4VTh8ywe5dM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hJ-jMXHX8QLr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of \"Attention is All You Need\"\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from onmt.encoders.encoder import EncoderBase\n",
        "from onmt.modules import MultiHeadedAttention\n",
        "from onmt.modules.position_ffn import PositionwiseFeedForward\n",
        "from onmt.utils.misc import sequence_mask\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the transformer encoder.\n",
        "\n",
        "    Args:\n",
        "        d_model (int): the dimension of keys/values/queries in\n",
        "                   MultiHeadedAttention, also the input size of\n",
        "                   the first-layer of the PositionwiseFeedForward.\n",
        "        heads (int): the number of head for MultiHeadedAttention.\n",
        "        d_ff (int): the second-layer of the PositionwiseFeedForward.\n",
        "        dropout (float): dropout probability(0-1.0).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, heads, d_ff, dropout, attention_dropout,\n",
        "                 max_relative_positions=0):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=attention_dropout,\n",
        "            max_relative_positions=max_relative_positions)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (FloatTensor): ``(batch_size, src_len, model_dim)``\n",
        "            mask (LongTensor): ``(batch_size, 1, src_len)``\n",
        "\n",
        "        Returns:\n",
        "            (FloatTensor):\n",
        "\n",
        "            * outputs ``(batch_size, src_len, model_dim)``\n",
        "        \"\"\"\n",
        "        input_norm = self.layer_norm(inputs)\n",
        "        context, _ = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                    mask=mask, attn_type=\"self\")\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.self_attn.update_dropout(attention_dropout)\n",
        "        self.feed_forward.update_dropout(dropout)\n",
        "        self.dropout.p = dropout\n",
        "\n",
        "\n",
        "class TransformerEncoder(EncoderBase):\n",
        "    \"\"\"The Transformer encoder from \"Attention is All You Need\"\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n",
        "\n",
        "    .. mermaid::\n",
        "\n",
        "       graph BT\n",
        "          A[input]\n",
        "          B[multi-head self-attn]\n",
        "          C[feed forward]\n",
        "          O[output]\n",
        "          A --> B\n",
        "          B --> C\n",
        "          C --> O\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): number of encoder layers\n",
        "        d_model (int): size of the model\n",
        "        heads (int): number of heads\n",
        "        d_ff (int): size of the inner FF layer\n",
        "        dropout (float): dropout parameters\n",
        "        embeddings (onmt.modules.Embeddings):\n",
        "          embeddings to use, should have positional encodings\n",
        "\n",
        "    Returns:\n",
        "        (torch.FloatTensor, torch.FloatTensor):\n",
        "\n",
        "        * embeddings ``(src_len, batch_size, model_dim)``\n",
        "        * memory_bank ``(src_len, batch_size, model_dim)``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, heads, d_ff, dropout,\n",
        "                 attention_dropout, embeddings, max_relative_positions):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.transformer = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(\n",
        "                d_model, heads, d_ff, dropout, attention_dropout,\n",
        "                max_relative_positions=max_relative_positions)\n",
        "             for i in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    @classmethod\n",
        "    def from_opt(cls, opt, embeddings):\n",
        "        \"\"\"Alternate constructor.\"\"\"\n",
        "        return cls(\n",
        "            opt.enc_layers,\n",
        "            opt.enc_rnn_size,\n",
        "            opt.heads,\n",
        "            opt.transformer_ff,\n",
        "            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n",
        "            opt.attention_dropout[0] if type(opt.attention_dropout)\n",
        "            is list else opt.attention_dropout,\n",
        "            embeddings,\n",
        "            opt.max_relative_positions)\n",
        "\n",
        "    def forward(self, src, lengths=None):\n",
        "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
        "        self._check_args(src, lengths)\n",
        "\n",
        "        emb = self.embeddings(src)\n",
        "\n",
        "        out = emb.transpose(0, 1).contiguous()\n",
        "        mask = ~sequence_mask(lengths).unsqueeze(1)\n",
        "        # Run the forward pass of every layer of the tranformer.\n",
        "        for layer in self.transformer:\n",
        "            out = layer(out, mask)\n",
        "        out = self.layer_norm(out)\n",
        "\n",
        "        return emb, out.transpose(0, 1).contiguous(), lengths\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.embeddings.update_dropout(dropout)\n",
        "        for layer in self.transformer:\n",
        "            layer.update_dropout(dropout, attention_dropout)\n"
      ]
    }
  ]
}