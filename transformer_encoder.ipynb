{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsKX8yM5aDpc",
        "outputId": "ca5491a9-6188-40cc-ea08-118334ad5ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install OpenNMT-py\n",
        "#!pip install -r requirements.opt.txt"
      ],
      "metadata": {
        "id": "Hzc4rtfvdBgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "C4VTh8ywe5dM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ-jMXHX8QLr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Implementation of \"Attention is All You Need\"\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from onmt.encoders.encoder import EncoderBase\n",
        "from onmt.modules import MultiHeadedAttention\n",
        "from onmt.modules.position_ffn import PositionwiseFeedForward\n",
        "from onmt.utils.misc import sequence_mask\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the transformer encoder.\n",
        "\n",
        "    Args:\n",
        "        d_model (int): the dimension of keys/values/queries in\n",
        "                   MultiHeadedAttention, also the input size of\n",
        "                   the first-layer of the PositionwiseFeedForward.\n",
        "        heads (int): the number of head for MultiHeadedAttention.\n",
        "        d_ff (int): the second-layer of the PositionwiseFeedForward.\n",
        "        dropout (float): dropout probability(0-1.0).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, heads, d_ff, dropout, attention_dropout,\n",
        "                 max_relative_positions=0):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=attention_dropout,\n",
        "            max_relative_positions=max_relative_positions)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (FloatTensor): ``(batch_size, src_len, model_dim)``\n",
        "            mask (LongTensor): ``(batch_size, 1, src_len)``\n",
        "\n",
        "        Returns:\n",
        "            (FloatTensor):\n",
        "\n",
        "            * outputs ``(batch_size, src_len, model_dim)``\n",
        "        \"\"\"\n",
        "        input_norm = self.layer_norm(inputs)\n",
        "        context, _ = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                    mask=mask, attn_type=\"self\")\n",
        "        out = self.dropout(context) + inputs\n",
        "        return self.feed_forward(out)\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.self_attn.update_dropout(attention_dropout)\n",
        "        self.feed_forward.update_dropout(dropout)\n",
        "        self.dropout.p = dropout\n",
        "\n",
        "\n",
        "class TransformerEncoder(EncoderBase):\n",
        "    \"\"\"The Transformer encoder from \"Attention is All You Need\"\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n",
        "\n",
        "    .. mermaid::\n",
        "\n",
        "       graph BT\n",
        "          A[input]\n",
        "          B[multi-head self-attn]\n",
        "          C[feed forward]\n",
        "          O[output]\n",
        "          A --> B\n",
        "          B --> C\n",
        "          C --> O\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): number of encoder layers\n",
        "        d_model (int): size of the model\n",
        "        heads (int): number of heads\n",
        "        d_ff (int): size of the inner FF layer\n",
        "        dropout (float): dropout parameters\n",
        "        embeddings (onmt.modules.Embeddings):\n",
        "          embeddings to use, should have positional encodings\n",
        "\n",
        "    Returns:\n",
        "        (torch.FloatTensor, torch.FloatTensor):\n",
        "\n",
        "        * embeddings ``(src_len, batch_size, model_dim)``\n",
        "        * memory_bank ``(src_len, batch_size, model_dim)``\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, heads, d_ff, dropout,\n",
        "                 attention_dropout, embeddings, max_relative_positions):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.transformer = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(\n",
        "                d_model, heads, d_ff, dropout, attention_dropout,\n",
        "                max_relative_positions=max_relative_positions)\n",
        "             for i in range(num_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    @classmethod\n",
        "    def from_opt(cls, opt, embeddings):\n",
        "        \"\"\"Alternate constructor.\"\"\"\n",
        "        return cls(\n",
        "            opt.enc_layers,\n",
        "            opt.enc_rnn_size,\n",
        "            opt.heads,\n",
        "            opt.transformer_ff,\n",
        "            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n",
        "            opt.attention_dropout[0] if type(opt.attention_dropout)\n",
        "            is list else opt.attention_dropout,\n",
        "            embeddings,\n",
        "            opt.max_relative_positions)\n",
        "\n",
        "    def forward(self, src, lengths=None):\n",
        "        \"\"\"See :func:`EncoderBase.forward()`\"\"\"\n",
        "        self._check_args(src, lengths)\n",
        "\n",
        "        emb = self.embeddings(src)\n",
        "\n",
        "        out = emb.transpose(0, 1).contiguous()\n",
        "        mask = ~sequence_mask(lengths).unsqueeze(1)\n",
        "        # Run the forward pass of every layer of the tranformer.\n",
        "        for layer in self.transformer:\n",
        "            out = layer(out, mask)\n",
        "        out = self.layer_norm(out)\n",
        "\n",
        "        return emb, out.transpose(0, 1).contiguous(), lengths\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.embeddings.update_dropout(dropout)\n",
        "        for layer in self.transformer:\n",
        "            layer.update_dropout(dropout, attention_dropout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer_decoder"
      ],
      "metadata": {
        "id": "eSKB3vhlXwT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Implementation of \"Attention is All You Need\"\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from onmt.decoders.decoder import DecoderBase\n",
        "from onmt.modules import MultiHeadedAttention, AverageAttention\n",
        "from onmt.modules.position_ffn import PositionwiseFeedForward\n",
        "from onmt.utils.misc import sequence_mask\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Transformer Decoder layer block in Pre-Norm style.\n",
        "    Pre-Norm style is an improvement w.r.t. Original paper's Post-Norm style,\n",
        "    providing better converge speed and performance. This is also the actual\n",
        "    implementation in tensor2tensor and also avalable in fairseq.\n",
        "    See https://tunz.kr/post/4 and :cite:`DeeperTransformer`.\n",
        "\n",
        "    .. mermaid::\n",
        "\n",
        "        graph LR\n",
        "        %% \"*SubLayer\" can be self-attn, src-attn or feed forward block\n",
        "            A(input) --> B[Norm]\n",
        "            B --> C[\"*SubLayer\"]\n",
        "            C --> D[Drop]\n",
        "            D --> E((+))\n",
        "            A --> E\n",
        "            E --> F(out)\n",
        "\n",
        "\n",
        "    Args:\n",
        "        d_model (int): the dimension of keys/values/queries in\n",
        "            :class:`MultiHeadedAttention`, also the input size of\n",
        "            the first-layer of the :class:`PositionwiseFeedForward`.\n",
        "        heads (int): the number of heads for MultiHeadedAttention.\n",
        "        d_ff (int): the second-layer of the :class:`PositionwiseFeedForward`.\n",
        "        dropout (float): dropout in residual, self-attn(dot) and feed-forward\n",
        "        attention_dropout (float): dropout in context_attn (and self-attn(avg))\n",
        "        self_attn_type (string): type of self-attention scaled-dot, average\n",
        "        max_relative_positions (int):\n",
        "            Max distance between inputs in relative positions representations\n",
        "        aan_useffn (bool): Turn on the FFN layer in the AAN decoder\n",
        "        full_context_alignment (bool):\n",
        "            whether enable an extra full context decoder forward for alignment\n",
        "        alignment_heads (int):\n",
        "            N. of cross attention heads to use for alignment guiding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, heads, d_ff, dropout, attention_dropout,\n",
        "                 self_attn_type=\"scaled-dot\", max_relative_positions=0,\n",
        "                 aan_useffn=False, full_context_alignment=False,\n",
        "                 alignment_heads=0):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        if self_attn_type == \"scaled-dot\":\n",
        "            self.self_attn = MultiHeadedAttention(\n",
        "                heads, d_model, dropout=attention_dropout,\n",
        "                max_relative_positions=max_relative_positions)\n",
        "        elif self_attn_type == \"average\":\n",
        "            self.self_attn = AverageAttention(d_model,\n",
        "                                              dropout=attention_dropout,\n",
        "                                              aan_useffn=aan_useffn)\n",
        "\n",
        "        self.context_attn = MultiHeadedAttention(\n",
        "            heads, d_model, dropout=attention_dropout)\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.full_context_alignment = full_context_alignment\n",
        "        self.alignment_heads = alignment_heads\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\" Extend `_forward` for (possibly) multiple decoder pass:\n",
        "        Always a default (future masked) decoder forward pass,\n",
        "        Possibly a second future aware decoder pass for joint learn\n",
        "        full context alignement, :cite:`garg2019jointly`.\n",
        "\n",
        "        Args:\n",
        "            * All arguments of _forward.\n",
        "            with_align (bool): whether return alignment attention.\n",
        "\n",
        "        Returns:\n",
        "            (FloatTensor, FloatTensor, FloatTensor or None):\n",
        "\n",
        "            * output ``(batch_size, T, model_dim)``\n",
        "            * top_attn ``(batch_size, T, src_len)``\n",
        "            * attn_align ``(batch_size, T, src_len)`` or None\n",
        "        \"\"\"\n",
        "        with_align = kwargs.pop('with_align', False)\n",
        "        output, attns = self._forward(*args, **kwargs)\n",
        "        top_attn = attns[:, 0, :, :].contiguous()\n",
        "        attn_align = None\n",
        "        if with_align:\n",
        "            if self.full_context_alignment:\n",
        "                # return _, (B, Q_len, K_len)\n",
        "                _, attns = self._forward(*args, **kwargs, future=True)\n",
        "\n",
        "            if self.alignment_heads > 0:\n",
        "                attns = attns[:, :self.alignment_heads, :, :].contiguous()\n",
        "            # layer average attention across heads, get ``(B, Q, K)``\n",
        "            # Case 1: no full_context, no align heads -> layer avg baseline\n",
        "            # Case 2: no full_context, 1 align heads -> guided align\n",
        "            # Case 3: full_context, 1 align heads -> full cte guided align\n",
        "            attn_align = attns.mean(dim=1)\n",
        "        return output, top_attn, attn_align\n",
        "\n",
        "    def _forward(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask,\n",
        "                 layer_cache=None, step=None, future=False):\n",
        "        \"\"\" A naive forward pass for transformer decoder.\n",
        "\n",
        "        # T: could be 1 in the case of stepwise decoding or tgt_len\n",
        "\n",
        "        Args:\n",
        "            inputs (FloatTensor): ``(batch_size, T, model_dim)``\n",
        "            memory_bank (FloatTensor): ``(batch_size, src_len, model_dim)``\n",
        "            src_pad_mask (LongTensor): ``(batch_size, 1, src_len)``\n",
        "            tgt_pad_mask (LongTensor): ``(batch_size, 1, T)``\n",
        "            layer_cache (dict or None): cached layer info when stepwise decode\n",
        "            step (int or None): stepwise decoding counter\n",
        "            future (bool): If set True, do not apply future_mask.\n",
        "\n",
        "        Returns:\n",
        "            (FloatTensor, FloatTensor):\n",
        "\n",
        "            * output ``(batch_size, T, model_dim)``\n",
        "            * attns ``(batch_size, head, T, src_len)``\n",
        "\n",
        "        \"\"\"\n",
        "        dec_mask = None\n",
        "\n",
        "        if step is None:\n",
        "            tgt_len = tgt_pad_mask.size(-1)\n",
        "            if not future:  # apply future_mask, result mask in (B, T, T)\n",
        "                future_mask = torch.ones(\n",
        "                    [tgt_len, tgt_len],\n",
        "                    device=tgt_pad_mask.device,\n",
        "                    dtype=torch.uint8)\n",
        "                future_mask = future_mask.triu_(1).view(1, tgt_len, tgt_len)\n",
        "                # BoolTensor was introduced in pytorch 1.2\n",
        "                try:\n",
        "                    future_mask = future_mask.bool()\n",
        "                except AttributeError:\n",
        "                    pass\n",
        "                dec_mask = torch.gt(tgt_pad_mask + future_mask, 0)\n",
        "            else:  # only mask padding, result mask in (B, 1, T)\n",
        "                dec_mask = tgt_pad_mask\n",
        "\n",
        "        input_norm = self.layer_norm_1(inputs)\n",
        "\n",
        "        if isinstance(self.self_attn, MultiHeadedAttention):\n",
        "            query, _ = self.self_attn(input_norm, input_norm, input_norm,\n",
        "                                      mask=dec_mask,\n",
        "                                      layer_cache=layer_cache,\n",
        "                                      attn_type=\"self\")\n",
        "        elif isinstance(self.self_attn, AverageAttention):\n",
        "            query, _ = self.self_attn(input_norm, mask=dec_mask,\n",
        "                                      layer_cache=layer_cache, step=step)\n",
        "\n",
        "        query = self.drop(query) + inputs\n",
        "\n",
        "        query_norm = self.layer_norm_2(query)\n",
        "        mid, attns = self.context_attn(memory_bank, memory_bank, query_norm,\n",
        "                                       mask=src_pad_mask,\n",
        "                                       layer_cache=layer_cache,\n",
        "                                       attn_type=\"context\")\n",
        "        output = self.feed_forward(self.drop(mid) + query)\n",
        "\n",
        "        return output, attns\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.self_attn.update_dropout(attention_dropout)\n",
        "        self.context_attn.update_dropout(attention_dropout)\n",
        "        self.feed_forward.update_dropout(dropout)\n",
        "        self.drop.p = dropout\n",
        "\n",
        "\n",
        "class TransformerDecoder(DecoderBase):\n",
        "    \"\"\"The Transformer decoder from \"Attention is All You Need\".\n",
        "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n",
        "\n",
        "    .. mermaid::\n",
        "\n",
        "       graph BT\n",
        "          A[input]\n",
        "          B[multi-head self-attn]\n",
        "          BB[multi-head src-attn]\n",
        "          C[feed forward]\n",
        "          O[output]\n",
        "          A --> B\n",
        "          B --> BB\n",
        "          BB --> C\n",
        "          C --> O\n",
        "\n",
        "\n",
        "    Args:\n",
        "        num_layers (int): number of encoder layers.\n",
        "        d_model (int): size of the model\n",
        "        heads (int): number of heads\n",
        "        d_ff (int): size of the inner FF layer\n",
        "        copy_attn (bool): if using a separate copy attention\n",
        "        self_attn_type (str): type of self-attention scaled-dot, average\n",
        "        dropout (float): dropout in residual, self-attn(dot) and feed-forward\n",
        "        attention_dropout (float): dropout in context_attn (and self-attn(avg))\n",
        "        embeddings (onmt.modules.Embeddings):\n",
        "            embeddings to use, should have positional encodings\n",
        "        max_relative_positions (int):\n",
        "            Max distance between inputs in relative positions representations\n",
        "        aan_useffn (bool): Turn on the FFN layer in the AAN decoder\n",
        "        full_context_alignment (bool):\n",
        "            whether enable an extra full context decoder forward for alignment\n",
        "        alignment_layer (int): NÂ° Layer to supervise with for alignment guiding\n",
        "        alignment_heads (int):\n",
        "            N. of cross attention heads to use for alignment guiding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, d_model, heads, d_ff,\n",
        "                 copy_attn, self_attn_type, dropout, attention_dropout,\n",
        "                 embeddings, max_relative_positions, aan_useffn,\n",
        "                 full_context_alignment, alignment_layer,\n",
        "                 alignment_heads):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        # Decoder State\n",
        "        self.state = {}\n",
        "\n",
        "        self.transformer_layers = nn.ModuleList(\n",
        "            [TransformerDecoderLayer(d_model, heads, d_ff, dropout,\n",
        "             attention_dropout, self_attn_type=self_attn_type,\n",
        "             max_relative_positions=max_relative_positions,\n",
        "             aan_useffn=aan_useffn,\n",
        "             full_context_alignment=full_context_alignment,\n",
        "             alignment_heads=alignment_heads)\n",
        "             for i in range(num_layers)])\n",
        "\n",
        "        # previously, there was a GlobalAttention module here for copy\n",
        "        # attention. But it was never actually used -- the \"copy\" attention\n",
        "        # just reuses the context attention.\n",
        "        self._copy = copy_attn\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "        self.alignment_layer = alignment_layer\n",
        "\n",
        "    @classmethod\n",
        "    def from_opt(cls, opt, embeddings):\n",
        "        \"\"\"Alternate constructor.\"\"\"\n",
        "        return cls(\n",
        "            opt.dec_layers,\n",
        "            opt.dec_rnn_size,\n",
        "            opt.heads,\n",
        "            opt.transformer_ff,\n",
        "            opt.copy_attn,\n",
        "            opt.self_attn_type,\n",
        "            opt.dropout[0] if type(opt.dropout) is list else opt.dropout,\n",
        "            opt.attention_dropout[0] if type(opt.attention_dropout)\n",
        "            is list else opt.dropout,\n",
        "            embeddings,\n",
        "            opt.max_relative_positions,\n",
        "            opt.aan_useffn,\n",
        "            opt.full_context_alignment,\n",
        "            opt.alignment_layer,\n",
        "            alignment_heads=opt.alignment_heads)\n",
        "\n",
        "    def init_state(self, src, memory_bank, enc_hidden):\n",
        "        \"\"\"Initialize decoder state.\"\"\"\n",
        "        self.state[\"src\"] = src\n",
        "        self.state[\"cache\"] = None\n",
        "\n",
        "    def map_state(self, fn):\n",
        "        def _recursive_map(struct, batch_dim=0):\n",
        "            for k, v in struct.items():\n",
        "                if v is not None:\n",
        "                    if isinstance(v, dict):\n",
        "                        _recursive_map(v)\n",
        "                    else:\n",
        "                        struct[k] = fn(v, batch_dim)\n",
        "\n",
        "        self.state[\"src\"] = fn(self.state[\"src\"], 1)\n",
        "        if self.state[\"cache\"] is not None:\n",
        "            _recursive_map(self.state[\"cache\"])\n",
        "\n",
        "    def detach_state(self):\n",
        "        self.state[\"src\"] = self.state[\"src\"].detach()\n",
        "\n",
        "    def forward(self, tgt, memory_bank, step=None, **kwargs):\n",
        "        \"\"\"Decode, possibly stepwise.\"\"\"\n",
        "        if step == 0:\n",
        "            self._init_cache(memory_bank)\n",
        "\n",
        "        tgt_words = tgt[:, :, 0].transpose(0, 1)\n",
        "\n",
        "        emb = self.embeddings(tgt, step=step)\n",
        "        assert emb.dim() == 3  # len x batch x embedding_dim\n",
        "\n",
        "        output = emb.transpose(0, 1).contiguous()\n",
        "        src_memory_bank = memory_bank.transpose(0, 1).contiguous()\n",
        "\n",
        "        pad_idx = self.embeddings.word_padding_idx\n",
        "        src_lens = kwargs[\"memory_lengths\"]\n",
        "        src_max_len = self.state[\"src\"].shape[0]\n",
        "        src_pad_mask = ~sequence_mask(src_lens, src_max_len).unsqueeze(1)\n",
        "        tgt_pad_mask = tgt_words.data.eq(pad_idx).unsqueeze(1)  # [B, 1, T_tgt]\n",
        "\n",
        "        with_align = kwargs.pop('with_align', False)\n",
        "        attn_aligns = []\n",
        "\n",
        "        for i, layer in enumerate(self.transformer_layers):\n",
        "            layer_cache = self.state[\"cache\"][\"layer_{}\".format(i)] \\\n",
        "                if step is not None else None\n",
        "            output, attn, attn_align = layer(\n",
        "                output,\n",
        "                src_memory_bank,\n",
        "                src_pad_mask,\n",
        "                tgt_pad_mask,\n",
        "                layer_cache=layer_cache,\n",
        "                step=step,\n",
        "                with_align=with_align)\n",
        "            if attn_align is not None:\n",
        "                attn_aligns.append(attn_align)\n",
        "\n",
        "        output = self.layer_norm(output)\n",
        "        dec_outs = output.transpose(0, 1).contiguous()\n",
        "        attn = attn.transpose(0, 1).contiguous()\n",
        "\n",
        "        attns = {\"std\": attn}\n",
        "        if self._copy:\n",
        "            attns[\"copy\"] = attn\n",
        "        if with_align:\n",
        "            attns[\"align\"] = attn_aligns[self.alignment_layer]  # `(B, Q, K)`\n",
        "            # attns[\"align\"] = torch.stack(attn_aligns, 0).mean(0)  # All avg\n",
        "\n",
        "        # TODO change the way attns is returned dict => list or tuple (onnx)\n",
        "        return dec_outs, attns\n",
        "\n",
        "    def _init_cache(self, memory_bank):\n",
        "        self.state[\"cache\"] = {}\n",
        "        batch_size = memory_bank.size(1)\n",
        "        depth = memory_bank.size(-1)\n",
        "\n",
        "        for i, layer in enumerate(self.transformer_layers):\n",
        "            layer_cache = {\"memory_keys\": None, \"memory_values\": None}\n",
        "            if isinstance(layer.self_attn, AverageAttention):\n",
        "                layer_cache[\"prev_g\"] = torch.zeros((batch_size, 1, depth),\n",
        "                                                    device=memory_bank.device)\n",
        "            else:\n",
        "                layer_cache[\"self_keys\"] = None\n",
        "                layer_cache[\"self_values\"] = None\n",
        "            self.state[\"cache\"][\"layer_{}\".format(i)] = layer_cache\n",
        "\n",
        "    def update_dropout(self, dropout, attention_dropout):\n",
        "        self.embeddings.update_dropout(dropout)\n",
        "        for layer in self.transformer_layers:\n",
        "            layer.update_dropout(dropout, attention_dropout)\n"
      ],
      "metadata": {
        "id": "funXLFM1XvTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}